---
layout:     post
title:      "Diffusion Policy 论文研读"
subtitle:   "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion 论文研读"
date:       2025-06-29 19:00:00
author:     "hyy"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - diffusion
---

本文提出了 **Diffusion Policy**，一种将机器人视觉运动策略表示为条件去噪扩散过程的新型机器人行为生成方法。通过学习动作分布分数函数的梯度，并在推理过程中通过随机朗之万动力学步骤迭代优化，该方法在 4 个机器人操作基准的 15 个任务中表现优异，平均性能比现有最先进方法提升 46.9%。其优势包括优雅处理多模态动作分布、适用于高维动作空间和出色的训练稳定性，同时引入了循环地平线控制、视觉条件和时间序列扩散 Transformer 等关键技术，为视觉运动策略学习开辟了新方向。

## 部分名词解释

- 机器人学习中的“多模态动作”

  同一状态下存在多种合理的动作选择，这些动作对应策略分布中的多个 “峰值”

  - 如机器臂推箱子左推还是右推
  - 无人机避障向上还是向左绕行

  本质是决策空间的多样性，与数据类型无关，而是策略对环境的灵活响应能力

- 监督回归任务

  通过已有的**输入 - 输出对（标注数据）**，学习一个从输入特征到连续输出值的映射函数

  - **输入**：通常是一组特征向量（如图像、传感器数据等），记为 $X$
  - **输出**：是连续的数值型变量（如坐标、速度、角度等），记为 $y$
  - **学习过程**：通过优化损失函数（如均方误差 MSE），拟合一个模型 $f: X \to y$，使得模型预测值 $\hat{y} = f(X)$ 与真实值 $y$ 尽可能接近

  机器人动作与传统监督回归相比具有特殊性：

  - **多模态分布**：同一观测可能对应多种有效动作，需模型捕捉多模态输出，而传统回归常假设单模态分布
  - **序列相关性**：机器人动作具有时间依赖性（如当前动作影响下一时刻观测），需考虑时序动态，而非独立样本
  - **高维与物理约束**：动作空间可能是高维的（如 $6$ 自由度机械臂），且需满足物理可行性（如关节限位、动力学约束），传统回归模型难以直接处理

- FiLM：逐特征线性调制

  一种**特征级别的条件调制机制**，实现 “用条件输入动态调整网络中间特征”

  不直接拼接或融合不同模态的特征，而是**利用 “条件输入”，对目标网络的中间特征进行逐特征的线性变换（缩放 + 偏移）** ，动态调整网络对特征的处理方式

  - 让条件输入决定哪些特征该放大、缩小甚至关闭，从而让模型根据不同条件灵活调整行为

- 因果交叉注意力：注意力机制，有**时序依赖**要求的任务里发挥作用

  - **交叉注意力**：让模型在处理序列时，能结合 “另一组序列” 的信息。如在机器人控制中，交叉注意力可让动作生成时参考观测信息，实现跨序列的特征交互
  - **因果注意力**：强制模型 “只能看历史信息，不能看未来信息”，保证序列生成的**时序因果性**

  把两者结合，让模型在跨序列交互时，严格遵循 “只能用历史信息” 的规则，既利用多序列关联，又保证时序逻辑合理

- 位置控制和速度控制

  - 位置控制：让执行机构精准运动到**指定的物理位置**
  - 速度控制：让执行机构稳定运行在**设定的旋转速度 / 线速度**

  位置控制和速度控制是紧密关联、协同作用的 “控制组合”

  - 位置控制：定义 “运动的终点与关键节点”

  - 速度控制：决定 “如何从一个位置移动到下一个位置”

    > 位置是目标，速度是过程

- 采样：从模型预测的动作概率分布中 “抽取具体动作指令” 的过程

- 逆行为克隆（隐式行为克隆，IBC）：用 “基于能量的隐式模型” 学习专家行为

  - 传统行为克隆把 “状态-动作” 当显示映射（如用神经网络直接学状态到动作的函数）
    - 序列决策任务中，误差会随时间累积，实际表现差

  - 隐式模型+能量函数，把 “模仿专家动作” 转化为能量最小化问题
    - 对专家演示的 “状态 - 动作对” 输出低能量，对随机 / 噪声动作输出高能量

    - 训练时，让模型学习 “给定状态 $S$ 下，能量最低的动作 $A$”

    - 适配高维动作空间、非连续 / 多模态行为

- 短视距多模态行为：指在**短时间尺度或局部步骤内**，智能体为实现**同一即时目标**，能够采取**多种不同动作策略或路径**的能力

  - 时间范围限制、目标一致性、策略多样性

- 长视距多模态行为：指智能体在**长期任务或复杂流程**中，以**不同顺序**完成**子目标**的能力

  - 时间跨度长、子目标顺序灵活、策略多样性（进阶）
  - 核心在于打破 “固定流程” 的限制，允许通过不同路径达成最终目标

- 端到端训练：将整个机器学习系统作为一个统一的整体进行优化，直接从原始输入映射到最终输出，无需手动设计中间处理步骤或分阶段训练组件

  - 传统方法通常将任务拆解为多个独立模块，每个模块单独训练，再串联使用
    - 如：先用预训练模型提取图像特征，再将特征输入策略网络生成动作
    - 端到端训练则省去中间模块，直接让模型学习 “输入→输出” 的端到端映射

  - 优化目标的一致性：分阶段训练的模块可能各自优化不同目标，导致整体系统非最优
  - 特征学习的任务针对性：预训练模型学习的是通用视觉特征，但机器人任务需要捕捉更精细的动态关系，端到端训练让模型自主发现 “哪些视觉特征对当前任务最重要”
  - 避免中间步骤的误差累积
  - 更好处理多模态与动态任务


## 引入

![strategy-comparison](/img/FPN/Diffusion-Policy/strategy-comparison.png)

1. 显示策略：直接显式地学习 “观测 → 动作” 的映射，输出是确定的或简单分布的动作
   - 输入是观测 $\mathbf{o}$（如机器人传感器数据、图像），输出是动作 $\hat{\mathbf{a}}$
   - 策略用函数 $F_\theta(\mathbf{o})$ 直接预测动作
   - 蓝色虚线框 → 动作分布的三种形式
     - **标量（回归）**：动作是单一连续值，用简单概率分布 $p(\mathbf{a})$ 建模
     - **高斯混合**：复杂动作分布可能由多个高斯分布叠加（如多模态运动）
     - **类别型（量化动作的分类表示）**：动作离散化
2. 隐式策略：用 “能量函数” 隐式建模动作分布，最优动作是能量最小化的结果，适合处理复杂多模态分布，但能量函数的优化可能很耗时
   - 输入是观测 $\mathbf{o}$ 和动作 $\mathbf{a}$，输出是 “能量” $E_\theta(\mathbf{o}, \mathbf{a})$（“动作有多好” 的打分）
   - 策略不直接预测动作，而是通过最小化能量 $\arg\min_\mathbf{a} E$ 间接找最优动作 $\hat{\mathbf{a}}$
   - 能量可视化
     - 右侧 “Energy” 图中，横轴是观测 $\mathbf{o}$，纵轴是动作 $\mathbf{a}$，颜色越浅表示能量越低（动作越好）
     - 黑色曲线是 “低能量轨迹”，表示在该观测下，哪些动作更优
3. 扩散策略：结合了隐式策略的 “能量建模” 和扩散模型的 “迭代去噪”，既能处理复杂动作分布，又能通过梯度引导高效优化动作，平衡了表达能力和计算效率
   - 输入是观测 $\mathbf{o}$ 和初始动作 $\mathbf{a}$（通常是随机噪声），输出是优化后的动作 $\hat{\mathbf{a}}$
   - 策略用扩散过程 $\mathcal{E}_\theta(\mathbf{o}, \mathbf{a})$ 迭代优化动作：通过计算能量梯度 $\nabla E(\mathbf{a})$，逐步 “去噪” 得到更优动作（类似扩散模型的反向过程）
   - 梯度场可视化
     - 右侧 “Gradient Field” 图中，箭头表示梯度方向（能量下降的方向）
     - 黑色曲线是动作优化的轨迹：从随机初始动作开始，沿着梯度方向迭代 $K$ 次，最终收敛到低能量的优质动作

**关键特性**

- 表达多模态动作分布

  学习动作分数函数的梯度，在该梯度场上进行随机朗之万动力学采样，表达任意可归一化的分布（表达多模态动作分布）

- 高维输出空间

  - 策略能够联合推断未来的动作序列，而不是单步动作
  - 促进动作的时间一致性和避免短视规划

- 稳定的训练

  学习能量函数的梯度，保持分布表达能力的同时实现了稳定的训练

**技术贡献**

- 闭环动作序列

  使策略能够以闭环方式持续重新规划动作

  - 策略能够以闭环方式持续重新规划动作，同时保持动作的时间一致性
  - 实现长时域规划与响应性之间的平衡

- 视觉条件处理

  - 引入了视觉条件扩散策略，将视觉观测作为条件而非联合数据分布的一部分
  - 无论去噪迭代次数如何，策略仅提取一次视觉表示，大幅减少了计算量并支持实时动作推理

- 时间扩散序列 transformer

  基于 Transformer 的扩散网络，可最小化典型 CNN 模型的过平滑效应，并在需要高频动作变化和速度控制的任务上实现了最先进的性能

## 研究现状

无需显式行为编程的智能机器人

### 行为克隆

- 操作任务
- 自动驾驶任务

#### 显式策略

1. **将世界状态或观测直接映射到动作**

   - 可以通过直接回归损失进行监督，并且只需一次前向传播即可实现高效推理

   - 不适合对多模态演示行为进行建模，且在高精度任务中表现不佳

2. **通过将动作空间离散化将回归任务转换为分类任务**
   - 近似连续动作空间所需的箱数会随维度增加呈指数增长

3. **结合分类分布和高斯分布，通过混合密度网络或带偏移预测的聚类方法来表示连续多模态分布**

   - 模型往往对超参数调整敏感，存在模式崩溃问题

   - 在表达高精度行为的能力上仍有局限

#### 隐式策略

通过基于能量的模型定义动作分布

- 在这种框架下，每个动作被赋予一个能量值，动作预测对应于寻找最小能量动作的优化问题（能够表示多模态分布）

- 在训练时稳定性不足（计算底层 Info-NCE 损失时需要抽取负样本）

#### 扩散模型

一类概率生成模型，通过迭代将随机采样的噪声逐步优化为符合目标分布的样本

- 可理解为学习隐式动作分数的梯度场，并在推理阶段对该梯度进行优化

**扩散模型的应用**

- 如何在规划场景中使用扩散模型，并推断可在给定环境中执行的动作轨迹
- 将扩散模型用于策略表示和基于状态观测的正则化
- 扩散模型如何有效应用于行为克隆场景，以构建高效的视觉 - 运动控制策略
- 从专家演示中学习的扩散模型如何用于增强经典显式策略，而不是直接将扩散模型用作策略表示

## 扩散策略形式化定义

如何将视觉运动机器人策略表述为去噪扩散概率模型（DDPMs）

### 1. 去噪扩散概率模型

一类生成模型，输出生成被建模为一个去噪过程（随机朗之万动力学）

从服从高斯噪声的采样值 $x^{K}$ 开始，DDPM 执行 $K$ 次去噪迭代，生成一系列噪声水平递减的中间动作 $x^{k}, x^{k-1} \dots x^{0}$，直至形成所需的无噪声输出 $x^{0}$
$$
x^{k-1}=\alpha\left(x^{k}-\gamma \varepsilon_{\theta}\left(x^{k}, k\right)+\mathscr{N}\left(0, \sigma^{2} I\right)\right)
$$

- $\varepsilon_{\theta}$ 是带有参数 $θ$ 的噪声预测网络（参数将通过学习进行优化）
- $\mathscr{N}(0, \sigma^{2} I)$ 是每次迭代中添加的高斯噪声

上述方程也可解释为单个含噪声的梯度下降步骤：
$$
x'=x-\gamma \nabla E(x)
$$

- 噪声预测网络 $\varepsilon_{\theta}(x, k)$ 有效预测梯度场 $\nabla E(x)$

- $γ$ 为学习率

噪声调度：将 $α$、$γ$ 和 $σ$ 选作迭代步长 $k$ 的函数，梯度下降过程中的学习率调度



![method](/img/FPN/Diffusion-Policy/method.png)

- 输入与输出
  - **输入**：视觉观测序列，包含机器人操作的图像和机器人位姿
  - **输出**：动作序列，即机器人需要执行的控制指令

- 扩散策略
  - **观测窗口**：策略输入是一段连续观测 $\mathbf{O}_t$（历史帧），输出是未来的动作序列 $\mathbf{A}_t$
  - **扩散过程**：动作序列从**高斯噪声 $\mathbf{A}^K$** 开始，经过 $K$ 次去噪迭代，逐渐收敛到**无噪声的优质动作 $\mathbf{A}^0$**
  - 基于 CNN 的扩散策略
    - FiLM 条件化：让观测特征 $\mathbf{O}_t$ 控制 CNN 的每一层，实现 “观测指导动作去噪”
      - 观测 $\mathbf{O}_t$ 经线性层输出两个参数 $a$,$b$
      - 这两个参数对**动作嵌入**逐通道进行线性调制 $a \cdot x + b$，相当于给每个通道的动作特征 “加权调整”
      - 调制后的特征通过 Conv1D 层，结合梯度 $\nabla E(\mathbf{A}_t)$ 完成一次去噪迭代，重复 $K$ 次得到最终动作
    - 适用于对空间特征敏感的任务（抓取）
  - 基于 Transformer 的扩散策略
    - 因果交叉注意力：让观测嵌入和动作嵌入高效交互，同时保证动作的**时序因果性**
      - 观测 $\mathbf{O}_t$ 嵌入和动作 $\mathbf{A}_t$ 嵌入输入 Transformer 解码器
      - 注意力掩码约束动作嵌入 “只看自己和之前的动作”，避免未来信息泄漏
      - 交叉注意力让观测和动作深度交互，结合梯度 $\nabla E(\mathbf{A}_t)$ 迭代去噪，生成最终动作序列
    - 适用于复杂时序任务（轨迹规划）

### 2. 去噪扩散概率模型训练

样本 $x_0$

- 随机选择一个去噪迭代步骤 $k$

- 为 $k$ 采样一个具有适当方差的随机噪声 $\varepsilon_k$

  要求噪声预测网络根据添加了噪声的数据样本预测噪声

- 损失函数：

$$
\mathcal{L} = \text{MSE}(\varepsilon_k, \varepsilon_\theta(x_0 + \varepsilon_k, k))
$$

### 3. 用于视觉运动策略学习的扩散模型

DDPM 如何修改用于机器人的视觉运动策略

- 将输出 $x$ 改为表示机器人动作
- 使去噪过程以输入观测 $O$ 为条件

#### 闭环动作序列预测

在长时域规划中促进时间一致性和平滑性，同时允许对意外观测做出快速反应

在重新规划前，将扩散模型生成的动作序列预测固定一段时间

在时间步 $t$，策略将最新的 $T_{0}$ 步观测数据 $O$ 作为输入，并预测 $T_{p}$ 步动作，其中 $T_{a}$ 步动作无需重新规划即可在机器人上执行

- $T_{0}$：观测时域
- $T_{p}$：动作预测时域
- $T_{a}$：动作执行时域

保持响应性的同时促进了动作的时间一致性

通过使用先前的动作序列预测对下一次推理设置进行热启动，进一步提高动作平滑性

#### 视觉观测条件化

使用 DDPM 来近似条件分布 $p(\mathbf{A}_t \mid \mathbf{O}_t)$

模型能够基于观测预测动作，无需推断未来状态

$$
\mathbf{A}_{t}^{k-1} = \alpha \left( \mathbf{A}_{t}^{k} - \gamma \varepsilon_\theta (\mathbf{O}_t, \mathbf{A}_{t}^{k}, k) + \mathcal{N}(0, \sigma^2 \mathbf{I}) \right)
$$

训练损失函数：

$$
\mathcal{L} = \text{MSE} \left( \varepsilon_k, \varepsilon_\theta (\mathbf{O}_t, \mathbf{A}_{t}^{0} + \varepsilon_k, k) \right)
$$

在去噪过程的输出中排除观测特征 $\mathbf{O}_t$

- 显著提升了推理速度并更好地适应实时控制

- 有助于实现视觉编码器的端到端训练

## 关键设计决策

### 1. 网络架构选择

为噪声预测网络 $\varepsilon_{\theta}$ 选择神经网络架构

#### 基于 CNN 的扩散策略

1D 时序 CNN

1. 逐特征线性调制（FiLM）

   将动作生成过程以观测特征 $O_t$ 和去噪迭代步骤 $k$ 为条件，仅对条件分布 $p(A_t \mid O_t)$ 进行建模

2. 仅预测动作轨迹，而非拼接的观测 - 动作轨迹

3. 移除了基于图像修复的目标状态条件化，使用与观测相同的 FiLM 条件化方法

当期望的动作序列随时间快速且剧烈变化时（如速度指令动作空间），其性能表现较差，这可能是由于时序卷积的归纳偏置更倾向于低频信号

过度平滑效应

#### 时间序列扩散 Transformer

minGPT 的 Transformer 架构进行动作预测

- 带噪声的动作 $A_t^k$ 作为输入标记传入 Transformer 解码器块
- 扩散迭代步骤 $k$ 的正弦嵌入作为首个标记前置输入
- 观测 $O_t$ 通过共享 MLP 转换为观测嵌入序列，随后作为输入特征传入 Transformer 解码器栈
- “梯度” $\varepsilon_\theta(O_t, A_t^k, k)$ 由解码器栈的每个对应输出标记预测

对超参数更为敏感，训练难度大

> 建议先用 CNN，如果性能不佳使用 Transformer。需要额外调参

### 2. 视觉编码器

将原始图像序列映射为潜在嵌入 $O_t$，并与扩散策略进行端到端训练

- 不同的相机视角使用独立的编码器，每个时间步的图像先被单独编码，再拼接形成 $O_t$

- 标准 ResNet-18（无预训练）作为编码器

  - 用空间 softmax 池化替换全局平均池化，以保留空间信息

  - 用组归一化替换批量归一化，以确保训练稳定性

    归一化层与指数移动平均结合使用

### 3. 噪声调度

噪声调度由 $σ$、$α$、$γ$ 以及作为 $k$ 函数的加性高斯噪声 $εₖ$ 定义

底层噪声调度控制着扩散策略捕捉动作信号高频和低频特征的程度

平方余弦调度在任务中效果最佳

### 4. 加速实时控制的推理过程

去噪扩散隐式模型（DDIM）方法将训练和推理中的去噪迭代次数解耦，从而允许算法在推理时使用更少的迭代次数来加速过程

实际实验中，使用 DDIM（训练迭代 100 次，推理迭代 10 次）可在 Nvidia 3080 GPU 上实现 0.1 秒的推理延迟

## 扩散策略的特性

### 1. 建模多模态动作分布

扩散策略能够自然且精确地表达多模态分布

- 潜在的随机采样过程

  扩散模型通过多次去噪迭代逐步优化动作序列，每次迭代都加入可控噪声（σ）。这种 “噪声 - 去噪” 循环允许模型在优化过程中从一个模态 “跳跃” 到另一个模态，最终收敛到最可能的解决方案之一

- 随机初始化引入多样性

  - 每次推理时，扩散模型从纯噪声（标准高斯分布）开始生成动作序列。不同的初始噪声样本会引导模型收敛到不同的动作模式
  - 这种随机性使模型能够探索多种可能的解决方案，而无需显式编码每个模式

- 隐式学习多模态分布

  通过训练学习数据中的真实分布，无需假设分布形式，使其能够自然地表示复杂的多峰分布

### 2. 与位置控制的协同效应

采用位置控制动作空间的扩散策略始终优于速度控制的扩散策略

- 位置控制模式下的动作多模态性比速度控制更为显著
- 位置控制相比速度控制受误差累积效应的影响更小，因此更适合动作序列预测

> 以哪种控制量为核心训练策略，更适配机器人运动任务
>
> - 以位置控制为动作空间
>   - 策略直接学习 “目标位置序列”，误差累积更小，且多模态动作更直观
> - 以速度控制为动作空间
>   - 速度是 “位置的导数”，易因积分误差累积导致位置偏差，且多模态动作（不同速度组合）不如位置模式清晰

### 3. 动作序列预测的优势

在大多数策略学习方法中，序列预测通常被规避，因为难以从高维输出空间中进行有效采样

序列预测难：

- 高维动作空间的 “维度灾难”

  - 空间规模指数级增长
  - 高维空间中，动作分布可能存在大量局部最优区域，无法找到全局最优动作序列

  > - 高维空间的 “体积爆炸” 使采样稀疏性加剧，大部分区域是无效动作，有效区域占比极低
  >
  > - 高维空间的 “距离度量失效” 导致模式识别困难
  >   - 在低维空间中，欧氏距离能有效衡量动作相似性
  >   - 无法准确判断两个动作是否属于同一模式（“距离集中” 现象）
  > - 序列预测的 “时序约束” 进一步缩小有效采样空间，相当于在高维动作空间中再叠加 “时序可行域” 的限制（如速度、加速度连续，不能突变）

- 序列依赖导致的 “时间维度耦合”

  - 动作序列的时序相关性：采样时需考虑历史动作的影响，不能独立采样每个时间步的动作
  - 错误累积效应：若某一步采样误差大，后续动作需基于错误状态调整，导致序列整体偏离目标

- 动作分布 “模式数量” 的不确定性

  - 多模式分布的建模困难：同一目标可能有多钟正确动作序列，且模式数量难以预先指定

DDPM（去噪扩散概率模型）在不牺牲模型表达能力的情况下，能很好地适应输出维度的扩展

- 动作时序一致性
- 对空闲动作的鲁棒性

### 4. 训练稳定性

<img src="/img/FPN/Diffusion-Policy/stability.png" alt="stability" style="zoom: 25%;" />

对比**扩散策略**与**隐式行为克隆**训练稳定性

- 真实场景动作预测误差
  - 扩散策略：预测精度稳步提升，训练过程稳定（误差持续降）
  - IBC：误差波动失控，可能因能量函数优化陷入局部震荡（非平滑能量景观）
- 仿真场景任务成功率
  - 扩散策略：训练过程中性能持续提升，最终收敛到稳定状态（成功率稳步升）
  - IBC：最佳检查点难选

> IBC 训练不稳定导致难以部署
>
> - 超参数调整
> - 检查点选择

扩散策略的推理和训练过程都不涉及对 $Z(o, \theta)$ 的评估，从而使扩散策略的训练更加稳定

### 5. 与控制理论的关联

用控制理论的线性系统案例，证明了扩散策略不是 “黑箱”，而是在简单场景下能与经典控制理论严格对齐，同时为复杂场景提供了更灵活的建模框架 

## 评估

### 1. 仿真环境与数据集

- Robomimic：用于研究模仿学习和离线强化学习的大规模机器人操作基准
- Push-T：需要利用复杂且富含接触的物体动力学，通过点接触精确推动 T 形块
- Multimodal Block Pushing：测试策略对多模态动作分布的建模能力（长时序多模态特性）
- Franka Kitchen：评估模仿学习和离线强化学习方法学习多任务长时序能力的常用环境

### 2. 评估方法

1. **成功率全面提升**
2. **检查点选择优势**：
   - Diffusion Policy 的平均性能更接近 max，无需复杂的检查点筛选
3. **对低质量数据的鲁棒性**：
   - 在 MH 数据下，Diffusion Policy 的性能衰减更慢，因其多模态建模能力可有效处理非熟练演示中的噪声

### 3. 关键发现

- 表达短视距多模态行为
- 表达长视距多模态行为
- 更好地利用位置控制（显著优于速度控制）
- 动作时域的权衡（动作序列预测的优势）
- 针对延迟的鲁棒性
  - 采用滚动时域位置控制来预测未来的动作序列，有助于解决由图像处理、策略推理和网络延迟导致的延迟间隙问题
  - 没有速度控制带来的累积误差
- 训练过程稳定
  - 最优超参数在不同任务中大多保持一致

### 4. 消融实验

探究**视觉编码器架构与训练策略对扩散策略性能的影响**

- ResNet-18、ResNet-34 和 ViT-B/16
- 端到端从头训练、使用冻结的预训练视觉编码器，以及微调预训练视觉编码器

以较小学习率微调预训练视觉编码器整体表现最佳

扩散策略的视觉性能对编码器架构和训练策略敏感，其中 “预训练 Transformer + 低学习率微调” 在数据效率和最终性能上表现最优，为机器人视觉系统的设计提供了具体的架构选择和训练范式参考

## 真实世界评估

### 1. Push-T 任务

- 真实世界 Push-T 任务为多阶段任务
  1. 将 T 形块推入目标区域
  2. 将末端执行器移动至指定终区
- 策略需进行精细调整，确保 T 形块完全位于目标区域，产生了额外的短视距多模态行为
- IoU 指标在最后一步测量，而非取所有步骤中的最大值

#### 结果分析

- 高度多模态特性
- 决策边界模糊：模型在对输入数据进行分类或决策时，不同类别或动作之间的界限不清晰、不明确，导致模型难以准确判断应该采取何种决策
  - 输入空间中类别重叠
  - 多模态特性导致决策歧义
  - 模型无法捕捉复杂特征关系

#### 端到端视觉编码器与预训练视觉编码器的对比

- 采用 R3M 预训练的扩散策略成功与端到端训练的版本相比，其预测的动作存在抖动且更容易卡住
- 采用 ImageNet 预训练的扩散策略表现更差，动作突兀且性能不佳
- 端到端训练仍是将视觉观测融入扩散策略的最有效方式，且所有表现最佳的模型均通过端到端训练获得

#### 针对扰动的鲁棒性

- 前摄像头被挥舞的手遮挡 3 秒
- 在扩散策略对 T 形块位置进行精细调整时，人为移动 T 形块
- 在第一阶段完成后机器人前往终区的过程中，移动 T 形块

### 2. 杯子翻转任务

将随机放置的杯子重新定向，使其满足：

1. 杯口朝下
2. 把手指向左侧

演示数据集具有高度多模态特性：

- 抓取与推动
- 不同类型的抓取方式（正手与反手）
- 局部抓取调整（绕杯子主轴旋转）

### 3. 酱汁倾倒与涂抹任务

测试扩散策略在真实场景中处理非刚性物体、六自由度动作空间以及周期性动作的能力

- 六自由度倾倒
  - 任务目标：将满满一勺酱汁倒在披萨面团中央
  - 性能：倾倒酱汁的掩码与面团中央标准圆形区域的交并比
- 周期性涂抹
  - 任务目标：在披萨面团上涂抹酱汁
  - 性能：酱汁覆盖面积来衡量

酱汁倾倒任务要求机器人保持静止一段时间，以便让长柄勺盛满粘稠的番茄酱：“闲置动作” 如何不被规避或过滤

如何实现自我终止

## 真实世界双手协作任务

观测空间与动作空间的改变

远程操作

### 1. 双手打鸡蛋任务

对于协调工具使用等日常任务，触觉反馈在双手操作远程控制中十分重要

- 若无触觉反馈，一位专家在 10 次尝试中未能成功完成一次演示
- 扩散策略在 20 次试验中以 55% 的成功率完成该任务

### 2. 双手铺卷垫任务

双向灵活性 —— 即根据初始条件，机器人可向左或向右展开垫子

### 3. 双手衬衫折叠任务

任务流程显著更长

## 局限性

- 实现继承了行为克隆的固有缺陷，例如在演示数据不足时性能欠佳

  扩散策略可应用于其他范式，以利用次优数据和负样本数据的优势

- 与 LSTM-GMM 等简单方法相比，扩散策略存在更高的计算成本和推理延迟

  未来研究可探索扩散模型加速技术的最新进展，以减少推理所需步骤