---
layout:     post
title:      "3D-aware 论文研读"
subtitle:   "3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification 论文研读"
date:       2025-07-06 19:00:00
author:     "hyy"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - VLN
---

本文提出了一种**3D-aware Object Goal Navigation 框架**，旨在解决未知环境中物体目标导航（ObjectNav）任务。该框架通过**在线语义点融合模块**构建 3D 场景表示，同时采用**角落引导探索政策**和**类别感知识别政策**两个子政策，分别实现环境探索与目标识别，结合局部规划模块完成导航。实验表明，该方法在 Gibson 和 Matterport3D 数据集上，相较于现有模块化方法，在 SPL、成功率等指标上达到最优，且训练计算成本降低**30 倍**，能以**15 FPS**运行。

## 部分名词解释

- 未知环境中的物体目标导航（ObjectNav）：要求智能体在一个未预先探索、未构建地图的场景中，自主找到并到达特定类别的物体（如椅子、沙发等）所在位置
- 在线离线：描述系统或流程与数据、任务交互方式的常见概念，核心区别在于是否实时处理数据或动态响应任务
  - 在线：强调与实时数据的动态交互和即时处理
    - 指系统或流程在运行过程中，实时接收输入数据并动态处理，同时根据实时反馈调整行为
    - 例如：在 ObjectNav 中，智能体通过传感器实时获取 RGB-D 图像和位姿信息，在线构建 3D 场景表示、更新导航目标，并根据新观测不断调整路径，整个过程与环境的交互是实时且动态的
  - 离线：侧重基于预设数据的预先处理和静态执行
    - 指系统或流程在处理任务前，已获取全部或大部分所需数据，通过预先处理、训练或计算来完成任务，过程中不依赖实时输入或动态调整
    - 例如：在导航任务中，若智能体先通过离线方式利用已有的场景数据训练模型，生成固定的导航策略，实际执行时直接调用该策略而不根据实时环境更新，则属于离线方式
  
- KL 散度：也称为相对熵，用来**衡量两个概率分布之间差异程度**的指标。核心作用是量化 “用一个概率分布去近似另一个概率分布时，所损失的信息”

  - 对于两个概率分布 $P$（真实分布）和 $Q$（近似分布），KL 散度的计算公式为：
    - $\text{KL}(P \parallel Q) = \sum_{x} P(x) \log\left( \frac{P(x)}{Q(x)} \right)$（离散分布）
    - $\text{KL}(P \parallel Q) = \int_{x} P(x) \log\left( \frac{P(x)}{Q(x)} \right) dx$（连续分布）


  > 不具有对称性

- 连续动作空间：动作空间中的动作由**连续的实数向量**表示，动作数量无限且不可数。智能体需要输出一个或多个连续值，直接对应物理动作（直接控制速度 / 角度）
  - 优势：动作表达精确，适合需要精细控制的场景
  - 劣势：
    1. **学习难度高**：智能体需要在无限的动作空间中找到最优解，训练不稳定
    2. **维度灾难**：若动作包含多个连续变量，搜索空间呈指数级增长
- 高维离散动作空间：动作空间中的动作由**有限个离散选项**组成，但选项数量非常大。智能体需要从这些离散选项中选择一个执行（选择地图上的离散目标格子）
  - 优势：将连续问题离散化，简化决策过程
  - 劣势：
    1. **维度爆炸**：随着地图分辨率提高，动作数量激增，训练效率骤降
    2. **稀疏奖励**：大量动作可能对目标无贡献（如选到障碍物所在格子），导致学习缓慢
- 快速行进法（FMM）：主要用于求解程函方程
  - **程函方程**：一般形式为 $\vert\nabla T(x)\vert = \frac{1}{F(x)}$，其中 $T(x)$ 是到达点 $x$ 的时间（或距离、代价等），$F(x)$ 是点 $x$ 处的速度
  - 算法流程：
    1. **初始化**：确定一个源点（例如在导航中，智能体的当前位置），将源点的到达时间设为 0，其他点的到达时间设为无穷大。同时，将源点加入到一个有序的活动点集合中
    2. **迭代更新**：从活动点集合中取出到达时间最小的点，称为当前点。根据当前点，更新其邻域点的到达时间（利用程函方程的离散化形式进行计算）。如果某个邻域点的到达时间被更新，且该点不在活动点集合中，则将其加入到活动点集合中
    3. **终止条件**：重复上述过程，直到所有需要计算的点的到达时间都被确定，或者满足特定的终止条件（比如目标点的到达时间已经被计算出来）


## 引入

物体目标导航（ObjectNav）

**现有研究**：

- 端到端强化学习

  - 以图像序列作为输入，并直接输出低级导航动作
  - 问题：样本效率较低，不同数据集间泛化性较差

- 基于模块化的方法

  - 语义场景映射模块：聚合 RGBD 观测数据和语义分割网络的输出以形成语义场景地图
  - 基于强化学习的目标策略模块：以语义场景地图作为输入，并学习在线更新目标位置
  - 局部路径规划模块：驱动智能体前往该目标位置

  > 场景地图的语义准确性和几何结构对物体目标导航的成功至关重要

现有的基于模块化的方法主要构建二维地图、场景图或神经场作为其场景地图

- 构建三维场景表示自然能比其二维对应物提供更准确、空间上更密集且更一致的语义预测
- 巨大挑战
  - 在整个楼层级场景中构建和查询细粒度的 3D 表示需要大量计算成本，这会显著减慢强化学习的训练速度
  - 与 2D 表示相比，3D 场景表示给目标策略带来的观测结果要复杂得多且维度更高，这会导致样本效率降低，并阻碍导航策略的学习

**新框架**：

- 用于 3D 语义场景映射的在线语义点融合模块
  - 在一种高效的在线点构建算法基础上扩展而来的，它能够从捕获的 RGBD 序列中实现在线语义融合和空间语义一致性计算
  - 内存效率更高
- 负责场景探索和物体识别的两个并行策略网络：探索策略和识别策略
  - 角落引导探索策略：生成探索目标
    - 通过学习预测场景边界框的四个角落之一作为长期离散目标，引导智能体感知周围环境，高效探索可能存在目标物体的区域
      - 场景边框：算法划定的一个虚拟边界框，大致框定了智能体当前可探索的场景范围
      - 智能体通过学习从这四个角落中选择一个作为长期目标，驱动自身向该方向移动，从而全面感知周围环境，避免在局部区域重复探索
  - 类别感知识别策略：在存在目标物体时生成已识别的物体目标
    - 动态学习离散的置信度阈值，用于识别每个类别的语义预测结果，提升目标识别的准确性
      - 智能体并非使用固定阈值（如所有类别都用 0.8），而是通过学习为不同物体类别生成专属的离散阈值
      - 当某个语义预测的置信度超过对应类别的阈值，且满足空间一致性条件（如周围点的语义标签一致）时，才会被判定为有效目标
      - 这种动态调整的方式能适应不同类别的识别难度（如易识别的可能用较低阈值，难识别的用较高阈值），减少误判，提升目标识别的准确性
- 局部路径规划模块
  - 根据是否有已识别目标，在这两个目标间切换输入

<img src="/img/VLN/3D-aware/process-overview.png" alt="process-overview" style="zoom:50%;" />

- 探索：A→B 阶段，智能体在探索策略的引导下寻找目标
- 识别：B→C 阶段，智能体持续识别出目标物体并最终发出停止指令

> 降低学习难度，提高性能

## 研究现状

### 1. 基于视觉序列的目标导航（GoalNav）

**端到端强化学习方法**：直接将 RGBD 序列作为输入，通过模型对环境进行隐式编码（不生成显式的地图等表示，而是通过网络内部参数学习环境特征），并直接输出低级导航动作（如前进、转向等）

- 视觉表征学习、设计辅助任务、采用数据增强等技术
- 性能优异
- 普遍存在训练样本效率低的问题，且迁移到真实世界时泛化能力有限

### 2. 基于显式场景表示的目标导航（GoalNav with Explicit Scene Representations）

减轻直接从视觉序列中学习的负担

**基于模块化的方法**

- 通过利用场景图或 2D 俯视图等显式场景表示，作为机器人观测的替代
- 具有更高的样本效率和更好的泛化能力
- 最新进展：基于边界的探索策略、幻觉驱动的语义映射方法以及新颖的验证阶段

### 3. 具有 3D 场景表示的具身人工智能任务

已有大量研究将 3D 场景表示应用于某些具身人工智能任务中

- 物体抓取、抽屉开启

- 强化学习、模仿学习、监督学习

  > - 仅在有限空间内执行
  > - 在大规模环境中，如 ObjectNav 中的楼层级场景，现有方法会受到复杂 3D 观测和高额计算成本的影响

## 方法

### 1. 任务定义与方法概述

- 物体目标导航任务

  初始化时，智能体随机定位，无法获取预构建的环境地图，且会得到一个目标类别 ID

  在每个时间步 t，智能体接收无噪声的机载传感器读数，包括以自身为中心的 RGB - D 图像，以及相对于 episode 起始点的 3 自由度位姿

  然后，智能体在由前进、左转、右转和停止组成的离散动作空间中，估计其用于移动的动作 $a_{t} \in A$

  给定 500 步的有限时间预算，当智能体距离指定类别的物体 1 米范围内时，终止移动 

![method](/img/VLN/3D-aware/method.png)

1. 输入：
   - **RGBD**：带深度信息的彩色图像，为智能体提供环境视觉输入
   - **Pose**：位姿信息，用于定位与场景融合
   - **Category ID**：目标物体类别 ID，明确导航任务
2. 基于点的场景表示
   - **$\text{Points}{^{(t)}}$**：时间步 t 的原始点云数据，来自 RGBD 深度信息解析。
   - **Fuse（融合）**：将连续时间步的点云融合，构建 **3D 场景表示 $\mathcal{M}_{3D}^{(t)}$**（空间完整的 3D 点云地图）
   - **Project（投影）**：把 3D 点云投影到 2D 平面，生成 **2D 语义地图 $\mathcal{M}_{2D}^{(t)}$**（简化计算，辅助导航决策）
3. 双策略模块
   - **角落引导探索策略（$\pi_e$）**： 基于场景拓扑，输出 **角落探索目标 $g_e^{(t)}$**，驱动智能体探索环境、扩大地图范围
   - **类别感知识别策略（$\pi_f$）**： 基于 3D/2D 场景表示，识别 “目标物体” 位置，输出 **目标物体目标 $g_f^{(t)}$**
     - 需满足两个条件才生效：
       - 识别置信度 > 阈值 $\tau^{(t)}$
       - 语义一致性检查
4. 执行模块
   - **Local Planning Module**：根据双策略输出的目标（一旦 $g_f^{(t)}$ 存在就会被设为智能体的逼近目标，无则会导航至长期角落目标 $g_e^{(t)}$），规划局部路径
   - **Action $(a_t)$**：输出智能体最终动作（如前进、转向），驱动其在环境中移动，完成导航

### 2. 面向导航的 3D 场景构建

在导航过程中，具备 3D 感知能力的智能体会持续通过传感器获取新的环境观测数据，并**逐步累加构建精细化的 3D 场景表示**（空间信息 + 语义信息），最终通过这些整合后的信息指导智能体的导航决策

楼层级目标导航需要处理的场景范围大、环境复杂：

- 若追求全面的 3D 场景表示，需处理海量数据，会导致计算成本激增
- 若简化 3D 表示，又可能丢失关键信息，影响导航精度

基于点的在线构建算法扩展：

- 在线组织 3D 点
- 语义融合
- 一致性估计

#### 1. 3D 场景表示

3D 场景在时间步 $t$ 被表示为点云 $P(t) = \{(P^{(t)}_l, P^{(t)}_s, P^{(t)}_c)\} \in \mathbb{R}^{N(t) \times (M+4)}$

- $N(t)$ 是当前点云中的总点数（随时间增加，因为智能体在不断观测新环境）

- $M+4$ 是每个点的 “属性维度”（即每个点包含 $M+4$ 个信息）

  - **位置信息**：$P^{(t)}_{i,l} \in \mathbb{R}^3$，即该点在 3D 空间中的坐标（$x,y,z$ 三个维度）
  - **语义信息**：$P^{(t)}_{i,s} \in \mathbb{R}^M$，即该点对应的物体类别标签（如 “椅子”“桌子”），用 $M$ 维向量表示（每个维度对应一个类别，值越高表示该点属于这个类别的概率越大）
  - **空间语义一致性信息**：$P^{(t)}_{i,c} \in \mathbb{R}^1$，即该点的语义标签与周围点的语义是否 “一致”（如一个标为 “椅子” 的点，周围是否也有大量 “椅子” 点），用一个数值表示一致性程度（值越高越可靠）

  > 3D 点的位置，属于什么物体，是否与周围环境匹配

#### 2. 在线 3D 点融合

如何实时更新点云

- 在每个时间步 $t$，智能体获取新的带位姿的 RGB 图像 $I^{(t)}_c$ 和深度图像 $I^{(t)}_d$
- 通过 “反投影” 操作，结合智能体自身的位姿，可以将 2D 深度图像中的每个像素 “还原” 到 3D 世界空间中，得到该像素对应的 3D 点的位置 $P^{(t)}_l$
- 新生成的 3D 点需要被组织到已有的点云中，形成连贯的场景表示。依赖一种 “基于点的构建算法”

<img src="/img/VLN/3D-aware/3D-point-fusion.png" alt="3D-point-fusion" style="zoom:50%;" />

- 机器人在未知环境中**主动移动探索**，通过多视角的传感器采集环境数据，逐步构建场景
- 机器人采集的环境数据被转化为**3D 点云**，并通过**动态块**和**点级八叉树**在线组织结构，支持快速查询

#### 3. 3D 点云的高效组织算法

1. 动态 3D 块分配

   - 算法将 3D 空间划分为多个边长固定的小立方体 “块”（$B_k$），每个块的边长为 10cm（**空间划分**）
   - 每个块只记录符合其边界范围的 3D 点（即点的 $x$、$y$、$z$ 坐标都在块的对应轴范围内），并通过 “树结构” 维护所有块的索引 $k$（**索引加速**）

   > 通过块索引 $k$，可以快速找到任意 3D 点 $p_i$ 所在的块，进而高效检索其周围的点，无需遍历整个场景的点云

2. 一级八叉树

   - 为每个 3D 点 $p_i$ 构建一个 “一级八叉树” $O_i$：将该点周围的空间分为 8 个象限，每个象限内找一个最近的点，然后将当前点与这 8 个点连接起来

   > 每个点能快速获取周围 8 个方向的 “邻居” 信息，从而捕捉细粒度的空间关系，提升场景理解的精度

#### 4. 在线语义融合

- 智能体在导航时，会从**多个视角**观测同一物体。每个视角的 2D 语义预测可能有差异，需要**融合多视角信息**，让 3D 点的语义标签更准确、一致

  如：智能体可能绕椅子一圈，从正面、侧面、背面拍了多张 RGBD 图像，每个视角预测 “这是椅子” 的概率可能不同

- **核心流程**：

  - 多视角语义预测

    - 同一个 3D 点 $p_i$，会被**不同时间 / 视角的 RGBD 帧**（$\{I_c^{(t)}, I_d^{(t)}\}$）观测到。每帧 RGBD 图像会输出一个 **2D 语义预测结果** $p_{i,s}^{(t)}(I_c^{(t)})$

    - 表示 “这一帧图像认为 $p_i$ 属于某类物体的概率”（比如第 $t$ 帧觉得 $p_i$ 是椅子的概率 70%，桌子 30%）

  - 取多视角预测的最大值

    - 对每个语义类别，取所有视角预测结果中的**最大值**
    - 再用归一化（$\mathcal{N}$）把这些最大值重新缩放成合理的概率分布（所有类别的概率和为 1）

> 相比于 3D 卷积和贝叶斯更新，内存时间效率高，提升语义精度

#### 5. 空间语义一致性

单个物体的所有点，语义标签应该一致（比如椅子的 3D 点云，每个点的语义都该是 “椅子”）：空间语义一致性信息 $P_{c}^{(t)}$ 作为 3D 场景表示的一部分，让智能体在导航时，不仅知道点的位置、语义，还知道 “语义是否自洽”

- **KL 散度**：衡量两个概率分布的差异。对比 “当前点 $P_i$ 的语义分布” 和 “其八叉树邻域点的语义分布”
- **八叉树**：每个点会连接笛卡尔坐标系 8 个象限的最近点（避免遍历全场景，提升效率）
- **计算逻辑**：对当前点 $P_i$，找它八叉树邻域里所有点 $P_j$，计算 $P_i$ 和每个 $P_j$ 的语义 KL 散度，**取最大值**作为 $P_{i,c}^{(t)}$

### 3. 同步探索与识别

两种互补的子策略：角落引导的探索策略和类别感知的识别策略

每种策略都通过学习预测低维度的离散动作，输出导航目标位置来引导智能体，从而在保证高性能的同时减少训练时间

#### 1. 观测空间

1. 3D 精细化观测 $x_{3D}^{(t)}$（3D 点云中采样的 4096 个点）

   $x_{3D}^{(t)} = \{P^{(t)} \in ((4 + m) \times N)\}$

   - **位置信息**：$p_l^{(t)} \in \mathbb{R}^3$
   - **融合语义信息**：$p_s^{(t)} \in \mathbb{R}^m$
   - **空间语义一致性**：$p_c^{(t)} \in \mathbb{R}^1$

2. 辅助观测：以自身为中心的 2D 全局地图 $x_{2D}^{(t)}$（“全局视野” 来规划大范围移动）

   $x_{2D}^{(t)} \in ((2 + m) \times M \times M)$

   - $M \times M$：地图尺寸
   - $2 + m$：地图的通道维度（地图包含 $2 + m$ 种信息）：
     1. **前 $2$ 个通道**：分别表示 “障碍物” 和 “已探索区域”（比如某网格是障碍物则标 1，否则 0；已探索过的区域标 1，未探索标 0）
     2. **后 $m$ 个通道**：每个通道对应一种物体类别（比如 “椅子” 通道中，某网格值越高，表示该位置有椅子的概率越大）

3. 额外输入：目标物体类别索引（OID）

   - 比如 “椅子” 的索引是 5，“桌子” 是 8
   - 明确智能体的导航目标（“我要找的是哪类物体”），让策略能针对性地探索（比如优先去可能有椅子的区域）和识别（重点关注语义为 “椅子” 的点）

#### 2. 角落引导的探索策略

引导智能体主动探索周围环境，直到接触到目标物体类别的任何实例

<img src="/img/VLN/3D-aware/corners-policy.png" alt="corners-policy" style="zoom:50%;" />

传统基于学习的探索策略：

- **动作空间复杂**：在 2D 地图上预测目标位置时，要么用 “连续动作空间”，要么用 “高维离散动作空间”
- **样本效率低**：复杂的动作空间会导致智能体需要学习的 “决策选项” 太多，需要海量训练样本才能学会有效探索，学习难度大、训练慢

启发式角落探索策略：用 “固定顺序的角落目标” 引导探索 → 会导致无效探索

**角落引导的探索策略**：$g_e = \pi_e(x_{3D}, x_{2D}, o_{ID}; \theta_e)$

- $\pi_e$：探索策略的函数，参数为 $\theta_e$
- 输入：3D 观测 $x_{3D}$、2D 地图 $x_{2D}$、目标物体类别索引 $o_{ID}$（即 “要找什么”）
- 输出：角落目标 $g_e$，即 2D 地图上**四个预定义角落**（左上、右上、左下、右下）中的一个

优点：

- **动作空间简化**：只让智能体从 “四个角落” 中选一个作为探索目标，动作空间维度降至 4 维离散
- **降低学习难度**：智能体需要学习的决策选项大幅减少，训练时需要的样本量显著降低，学习效率提升
- **避免无效探索**：基于角落目标的探索策略能**避免 “来回踱步”**，让智能体更有方向性地向未探索区域推进，提升探索效率

> - 四个角落是高效覆盖环境的最优解
>   - 角落天然适合探索
>   - 角落可减少探索冗余：避免踱步，系统性覆盖未探索区域
> - 角落目标基于学习预测，动态判断哪个角落最有价值：有大量未探索区域 + 语义可能包含目标物体

#### 3. 类别感知识别策略

1. 传统方法的问题：

   1. 传统方法用**固定阈值**（比如不管什么物体、什么视角，都用 0.7 的置信度判断是否为目标），但实际中：

      - 不同物体类别的语义预测难度不同（比如 “椅子” 容易识别，“植物” 难识别）
      - 不同观测角度的预测结果差异大（比如正面拍椅子识别准，侧面拍可能识别差）

      固定阈值无法适应这些变化，可能导致漏检和误检

   2. 传统方法只看单个点的语义置信度，忽略 3D 空间的语义一致性

2. **解决方法**：

   1. **动态置信度阈值**：$s = \pi_f(x_{3D}, o_{ID}; \theta_f)$

      - 输入：3D 观测 $x_{3D}$、目标类别索引 $o_{ID}$
      - 输出：离散动作 $s \in \{0,1,...,9\}$（共 10 个选项，用来动态调整阈值）

      **阈值计算**：$\tau = \tau_{\text{low}} + s \cdot \frac{1 - \tau_{\text{low}}}{10}$

      - $\tau_{\text{low}} = 0.5$（最低阈值），所以阈值范围是 $\tau \in [0.5, 0.95]$（$s=0$ 时 $\tau=0.5$，$s=9$ 时 $\tau=0.95$）
      - 智能体通过学习，根据当前环境和目标类别，选择合适的 $s$，动态调整识别阈值

   2. **空间语义一致性校验**：即使通过动态阈值识别出 “可能是目标” 的点，还需要进一步校验**这些点在 3D 空间中的语义是否一致**，避免误识别

      - 用点级八叉树 $O_p$ 找到目标点 $p$ 的邻域点 $\{p_i | (p_i, p) \in O_p\}$，近似表示目标物体的 3D 表面
      - 要求这些邻域点的语义标签**一致**（比如目标是椅子，邻域点也得是椅子或合理关联的类别）
      - 进一步检查 “2 环邻居”（邻域点的邻域点），确保语义一致性延伸到更大范围

<img src="/img/VLN/3D-aware/semantic-consistency.png" alt="semantic-consistency" style="zoom:50%;" />

 (A)：初始融合点云（多类别混合）

- **元素**：不同颜色的点代表不同语义类别，点之间的连线表示八叉树邻域关系

(B)：用动态阈值筛选目标类别点（仅保留 “目标类别置信度 $≥$ 动态阈值 $\tau$” 的点）

(C)：校验空间语义一致性，确认目标

- 红色实心圆：最终确认的目标
- 红色 1 环点：目标点的直接八叉树邻域点
- 红色 2 环点：1 环点的八叉树邻域点（即目标点的 “邻域的邻域”）

#### 4. 局部规划模块

接收两种策略输出的目标（$g_e$ 和 $g_f$），然后生成具体的移动路径，指导智能体行动

优先找已识别的目标，否则探索：

- **目标定义**：

  - $g_f$：识别策略输出的 “目标物体位置”（比如已经确认的椅子位置）
  - $g_e$：探索策略输出的 “角落目标”（用于引导智能体探索未知区域）

- **优先级逻辑**：

  1. 如果已经识别到目标物体（$g_f$ 存在），智能体**优先前往 $g_f$**（直接去目标位置，完成导航任务）
  2. 如果还没识别到目标（$g_f$ 不存在），智能体**采用 $g_e$**（先去角落探索，直到找到目标）

  > 避免无效踱步徘徊

路径规划方法：快速行进法

- 在 2D 地图 $M_{2D}$ 上，从智能体当前位置到目标位置（$g_f$ 或 $g_e$），规划出**避开障碍物的最短路径**

路径规划完成后，智能体按路径执行 “确定性动作”，保证精准沿路径移动，避免偏离

#### 5. 奖励机制

强化学习中**奖励机制**的设计，引导探索策略和识别策略学习正确的行为

**探索策略的奖励设计（鼓励高效探索未知区域）**：

1. 稀疏成功奖励（$r_{\text{success}} = 2.5$）

- “**稀疏**” 含义：仅在智能体**成功找到目标物体**时才给予奖励，其他时候无此奖励，避免智能体无目的漫游

2. 松弛奖励（$r_{\text{slack}} = 10^{-2}$）

- “**松弛**” 含义：每一步行动都给予少量奖励（无论是否探索到新区域）
- 作用：鼓励智能体 “持续移动”，间接推动其向未探索区域前进（原地不动无奖励，一直停在原地无法获得后续奖励）

3. 探索奖励（$r_{\text{explore}} = n_{p}^{\text{new}} \times 10^{-3}$）

- “**密集**” 含义：每一步行动后，根据 “新插入的 3D 点数量”（$n_{p}^{\text{new}}$）给予实时奖励（探索到的新点越多，奖励越高）
  - 例如：智能体移动后新观测到 1000 个 3D 点，探索奖励为 $1$

> - 直接鼓励智能体 “探索未观测区域”（新点越多，奖励越高）
> - 让智能体学会 “选择最有效的方向”（比如朝角落移动能观测到更多新点，而原地打转几乎无新点）

**识别策略的奖励设计（鼓励精准识别目标）**

1. **成功奖励（$r_{\text{success}} = 2.5$）**：与探索策略一致，仅在**成功识别并定位目标物体**时给予，强化 “精准识别” 的最终目标
2. **松弛奖励（$r_{\text{slack}} = 0.01$）**：每一步行动给予少量奖励，避免智能体在识别过程中停滞（比如一直不判断目标是否存在）

> 保证与探索策略的一致性（避免目标冲突），又简化学习难度（无需额外设计复杂奖励）

## 实验

### 1. 实验设置

在 Matterport3D（MP3D）和 Gibson 数据集上，使用 Habitat 模拟器进行实验

- 对于 Gibson 数据集，采用 Gibson tiny 分集中的 25 个训练场景和 5 个验证场景，考虑 6 个目标类别，包括椅子、沙发、盆栽、床、马桶和电视
- 对于 MP3D 数据集，使用 Habitat ObjectNav 数据集的标准划分，包含 61 个训练场景和 11 个验证场景，涉及 21 个目标类别

**实现细节**：

1. **数据输入**：RGB-D 图像 → 2D 语义模型（RedNet/Mask R-CNN）→ 语义分割结果
2. **特征提取**：
   - 3D 点云：随机采样 512 点 → PointNet 提取特征
   - 2D 地图：全卷积网络提取特征
3. **策略决策**：结合 3D 点特征、2D 地图特征 → PPO 训练的策略网络（$\pi_e$/$\pi_f$）→ 输出动作（如选择角落目标、调整识别阈值）
4. **训练优化**：每 25 步更新一次策略，通过奖励机制（如探索奖励、成功奖励）引导策略学习最优行为

**评估指标**：

1. SPL（成功率加权路径长度）
2. 成功率：成功完成任务的回合所占百分比
3. Soft SPL（软成功率加权路径长度）：SPL 的弱化版本，用于衡量向目标推进的进度（即使成功率为 0 也适用）
4. DTS（最终成功距离）：回合结束时与成功状态的测地线距离（单位：米）

**baseline**：

- **端到端 RL**：DD-PPO、Red-Rabbit、THDA、Habitat-Web
- **模块化方法**：FBE、ANS、L2M、SemExp、Stubborn、PONI

### 2. 结果

<img src="/img/VLN/3D-aware/visualization.png" alt="visualization" style="zoom:50%;" />

论文方法在**MP3D 数据集**中 “寻找床” 任务的**定性可视化**，完整展示了智能体从 “误判目标” 到 “纠正错误”，最终 “成功识别并到达目标” 的全过程

地图类型：

- **3D KL Map**：用颜色（从蓝到红）表示 “语义不确定性”（蓝色 = 确定，红色 = 不确定）
- **3D Semantic Map**：用颜色编码物体类别，可视化智能体观测到的语义信息

**流程**：

1. 步骤 10：误判发生

   - **场景**：智能体观测到一个 “类似床” 的区域，但实际是沙发 + 靠垫的组合（干扰物）

   - **结果**：智能体开始向该区域移动

2. 步骤 11：排除错误

   - **场景**：智能体接近红框区域，发现语义预测矛盾（实际是沙发，绿色为主）

   - **空间一致性校验**：通过 “2 环邻域语义一致性” 检查，发现该区域的语义标签（沙发）与 “床” 不一致，排除误判

   - **结果**：错误被修正，智能体调整方向，继续探索

3. 步骤 12：错误修正完成

   - **场景**：智能体离开干扰区域，语义地图中红框消失，绿色（沙发）标签确认

   - **意义**：证明动态阈值 + 空间一致性校验有效过滤了误识别，避免智能体被干扰物误导

4. 步骤 157：智能体观测到真实床的区域，语义预测开始出现红色
5. 步骤 158：更多点被预测为 “床”（红色区域扩大），通过空间一致性校验（邻域点语义一致）
6. 步骤 167：智能体到达目标区域，红色区域完全覆盖床，识别成功，停止移动

**核心机制**：

1. **动态语义预测更新**：从 Step 10 到 Step 11，语义预测从 “错误红” 变为 “正确绿”，体现**在线语义融合**和**动态阈值筛选**（过滤低置信度预测）
2. **空间语义一致性校验**：Step 10 的错误区域因 “邻域语义混乱” 被排除，Step 157-167 的真实床因 “邻域语义一致” 被确认，体现**2 环邻域校验**（避免单个误识别点干扰）
3. **探索与识别的协同**：智能体先被干扰物吸引（探索策略驱动），但通过识别策略修正方向，最终找到目标，体现**探索策略（找未知区域）+ 识别策略（精准定位目标）** 的协同机制

---

**探索策略对比**

角落引导式探索策略优于主流现有方法，包括基于学习的方法和启发式方法

研究发现，通过学习从场景的四个角落中预测离散的角落目标，可实现最佳性能。表明四角落设计已能够高效引导智能体探索环境

---

**识别策略对比**

<img src="/img/VLN/3D-aware/3D-point-cloud.png" alt="3D-point-cloud" style="zoom:50%;" />

仅通过使用基于 3D 点的构建和融合算法，性能就有所提升

表明多视角观测能提供更准确的语义预测，有效减少假阳性预测

此外，类别感知识别策略通过预测动态阈值，表现出了更优的性能

<img src="/img/VLN/3D-aware/dynamic-threshold.png" alt="dynamic-threshold" style="zoom:50%;" />

- **Table（桌子，浅蓝色）**：成功率 52.6%（易识别），阈值分布广（0.5 - 0.9 均有覆盖，低阈值占比不低 ）→ 策略对易识别类别 “放宽标准”，避免漏检
- **Cushion（靠垫，橙色）**：成功率 36.9%（较难识别），阈值集中在 0.7 - 0.9→ 策略对中等难度类别 “适度严格”，平衡漏检和误判
- **Plant（植物，红色）**：成功率 16.1%（最难识别），阈值集中在 0.8 - 1.0→ 策略对难识别类别 “严格把关”（高阈值占比高），减少假阳性误判

说明：

- **类别感知特性**：不同难度类别（易 / 中 / 难）对应不同的阈值分布策略，证明方法能 “自适应调整严格程度”
- 核心逻辑：
  - 易识别类别（如桌子）：低阈值允许更多预测，优先保证 “找到目标”（即使有少量误判，也能通过空间校验过滤）
  - 难识别类别（如植物）：高阈值严格筛选预测，优先保证 “预测准确”（避免大量假阳性干扰导航）

---

**消融实验**：验证方法中不同组件的有效性

**内存效率高**
